# Text-and-Code-Generator-using-GPT-Neo

This project harnesses the capabilities of the GPT Neo model(via Hugging Face Transformers) to generate human-like text and code.  This project showcases how GPT Neo can be employed to generate content across various domains, from creative writing to programming.

The [GPTNeo model](https://huggingface.co/docs/transformers/en/model_doc/gpt_neo) was released in the EleutherAI/gpt-neo repository by Sid Black, Stella Biderman, Leo Gao, Phil Wang and Connor Leahy. It is a GPT2 like causal language model trained on the Pile dataset. The architecture is similar to GPT2 except that GPT Neo uses local attention in every other layer with a window size of 256 tokens.
